{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e5548b35",
   "metadata": {},
   "source": [
    "# Clickbait Challenge at SemEval 2023 - Clickbait Spoiling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20f2cef5",
   "metadata": {},
   "source": [
    "Task 1 on Spoiler Type Classification: The input is the clickbait post and the linked document. The task is to classify the spoiler type that the clickbait post warrants (either \"phrase\", \"passage\", \"multi\"). For each input, an output like ```{\"uuid\": \"<UUID>\", \"spoilerType\": \"<SPOILER-TYPE>\"}``` has to be generated where <SPOILER-TYPE> is either phrase, passage, or multi.\n",
    "    \n",
    "For each entry in the training and validation dataset, the following fields are available:\n",
    "\n",
    "* uuid: The uuid of the dataset entry.\n",
    "* postText: The text of the clickbait post which is to be spoiled.\n",
    "* **targetParagraphs**: The main content of the linked web page to classify the spoiler type ***(task 1)*** and to generate the spoiler (task 2). Consists of the paragraphs of manually extracted main content.\n",
    "* **targetTitle**: The title of the linked web page to classify the spoiler type ***(task 1)*** and to generate the spoiler (task 2).\n",
    "* targetUrl: The URL of the linked web page.\n",
    "* humanSpoiler: The human generated spoiler (abstractive) for the clickbait post from the linked web page. This field is only available in the training and validation dataset (not during test).\n",
    "* spoiler: The human extracted spoiler for the clickbait post from the linked web page. This field is only available in the training and validation dataset (not during test).\n",
    "* spoilerPositions: The position of the human extracted spoiler for the clickbait post from the linked web page. This field is only available in the training and validation dataset (not during test).\n",
    "* **tags**: The spoiler type (might be \"phrase\", \"passage\", or \"multi\") that is to be classified in ***task 1*** (spoiler type classification). For task 1, this field is only available in the training and validation dataset (not during test). For task 2, this field is always available and can be used.\n",
    "\n",
    "Some fields contain additional metainformation about the entry but are unused: postId, postPlatform, targetDescription, targetKeywords, targetMedia."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caa4d02b",
   "metadata": {},
   "source": [
    "## Deep models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bf3c343",
   "metadata": {},
   "source": [
    "### Training process"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44b454bd",
   "metadata": {},
   "source": [
    "Importing all needed libraries and defining some custom functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d2780708",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset,Dataset,DatasetDict\n",
    "from transformers import DataCollatorWithPadding,AutoModelForSequenceClassification, Trainer, TrainingArguments,AutoTokenizer,AutoModel,AutoConfig\n",
    "from transformers.modeling_outputs import TokenClassifierOutput\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "\n",
    "def create_df_from_jsonl(path):\n",
    "    df = pd.read_json(path, lines=True)\n",
    "    df['input'] = df['postText'].apply(', '.join) + '. ' + df['targetParagraphs'].apply(', '.join)\n",
    "    df['label'] = df['tags'].apply(', '.join)\n",
    "    return df[['input', 'label']]\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20d1781a",
   "metadata": {},
   "source": [
    "Uploading data and preprocessing it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "103520a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = create_df_from_jsonl('data/train.jsonl')\n",
    "train_df = train_df[train_df.label != \"multi\"]\n",
    "train_df['label']= pd.get_dummies(train_df['label'],drop_first=True)['phrase']\n",
    "\n",
    "test_df = create_df_from_jsonl('data/validation.jsonl')\n",
    "test_df = test_df[test_df.label != \"multi\"]\n",
    "test_df['label']= pd.get_dummies(test_df['label'],drop_first=True)['phrase']\n",
    "\n",
    "train_df = Dataset.from_pandas(train_df)\n",
    "test_df = Dataset.from_pandas(test_df)\n",
    "\n",
    "train_df = train_df.remove_columns(['__index_level_0__'])\n",
    "test_df = test_df.remove_columns(['__index_level_0__'])\n",
    "data = DatasetDict({\n",
    "    'train': train_df,\n",
    "    'test': test_df})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "957143dd",
   "metadata": {},
   "source": [
    "Downloading pretrained model and using its tokenizer to tokenize data.\n",
    "Other models that were trained were:\n",
    "- bert-base-uncased with batch_size equal to 4\n",
    "- microsoft/deberta-base-mnli with batch_size equal to 1\n",
    "- textattack/roberta-base-MNLI with batch_size equal to 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2059373a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter 'function'=<function tokenize at 0x000001EE2B37E288> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d538dda44d2549339202cbf035ddfae7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b7de783931a46a4ae4f59476d397e7d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "checkpoint = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "tokenizer.model_max_len=512\n",
    "\n",
    "def tokenize(batch):\n",
    "    return tokenizer(batch[\"input\"], truncation=True,max_length=512)\n",
    "\n",
    "tokenized_dataset = data.map(tokenize, batched=True)\n",
    "tokenized_dataset.set_format(\"torch\",columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1f657f0",
   "metadata": {},
   "source": [
    "Defining model structure and its fuctions that will be used for training and evaluating model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "309e6f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomModel(nn.Module):\n",
    "    def __init__(self,checkpoint,num_labels): \n",
    "        super(CustomModel,self).__init__() \n",
    "        self.num_labels = num_labels \n",
    "\n",
    "        #Load Model with given checkpoint and extract its body\n",
    "        self.model = model = AutoModel.from_pretrained(checkpoint,config=AutoConfig.from_pretrained(checkpoint, output_attentions=True,output_hidden_states=True))\n",
    "        self.dropout = nn.Dropout(0.1) \n",
    "        self.classifier = nn.Linear(768,num_labels) # load and initialize weights\n",
    "\n",
    "    def forward(self, input_ids=None, attention_mask=None,labels=None):\n",
    "        #Extract outputs from the body\n",
    "        outputs = self.model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "\n",
    "        #Add custom layers\n",
    "        sequence_output = self.dropout(outputs[0]) #outputs[0]=last hidden state\n",
    "\n",
    "        logits = self.classifier(sequence_output[:,0,:].view(-1,768)) # calculate losses\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss_fct = nn.CrossEntropyLoss()\n",
    "            loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
    "\n",
    "        return TokenClassifierOutput(loss=loss, logits=logits, hidden_states=outputs.hidden_states,attentions=outputs.attentions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94fdbc1d",
   "metadata": {},
   "source": [
    "Setting the hiperparameters for training process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0ba7f388",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "C:\\Users\\Kubi\\anaconda3\\envs\\p37\\lib\\site-packages\\transformers\\optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  FutureWarning,\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model=CustomModel(checkpoint=checkpoint,num_labels=2).to(device)\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    tokenized_dataset[\"train\"], shuffle=True, batch_size=4, collate_fn=data_collator\n",
    ")\n",
    "eval_dataloader = DataLoader(\n",
    "    tokenized_dataset[\"test\"], shuffle=True, batch_size=4, collate_fn=data_collator\n",
    ")\n",
    "from transformers import AdamW,get_scheduler\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "num_epochs = 10\n",
    "num_training_steps = num_epochs * len(train_dataloader)\n",
    "lr_scheduler = get_scheduler(\n",
    "    \"linear\",\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=num_training_steps,\n",
    ")\n",
    "from datasets import load_metric\n",
    "metric = load_metric('f1')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff7f52c9",
   "metadata": {},
   "source": [
    "Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70fe1edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "progress_bar_train = tqdm(range(num_training_steps))\n",
    "progress_bar_eval = tqdm(range(num_epochs * len(eval_dataloader)))\n",
    "\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for batch in train_dataloader:\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        progress_bar_train.update(1)\n",
    "\n",
    "    model.eval()\n",
    "    for batch in eval_dataloader:\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**batch)\n",
    "\n",
    "        logits = outputs.logits\n",
    "        predictions = torch.argmax(logits, dim=-1)\n",
    "        metric.add_batch(predictions=predictions, references=batch[\"labels\"])\n",
    "        progress_bar_eval.update(1)\n",
    "    \n",
    "    print(metric.compute())\n",
    "\n",
    "      "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "035b98ab",
   "metadata": {},
   "source": [
    "### Evaluating the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f53080f8",
   "metadata": {},
   "source": [
    "All trained models are saved on hard disc with paths:\n",
    "- output/roberta\n",
    "- output/deberta\n",
    "- output/bert-best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "daaee473",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(\"output/bert-best\")\n",
    "from transformers import pipeline\n",
    "evaluate_model = pipeline('text-classification', model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5dc6e6a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "text1 = \"This Is How Many People Police Have Killed So Far In 2016In the first half of 2016, police have killed 532 people — many of whom were unarmed, mentally ill, and people of color. Going by the Going by the Guardian’s count , 261 white people were killed by police — the highest total out of any racial group. But data also shows that black people and Native Americans are being killed at higher rates than any other group. The slight discrepancies in numbers between Killed by Police and The Guardian reflect differences in how each outlet collects data about police killings. Killed by Police is mainly open-sourced and also relies on The slight discrepancies in numbers between Killed by Police and The Guardian reflect differences in how each outlet collects data about police killings. Killed by Police is mainly open-sourced and also relies on corporate news reports for its data on people killed by police. For its database, The Guardian relies on traditional reporting on police reports and witness statements, while also culling data from verified crowdsourced information using regional news outlets, research groups, and reporting projects that include Killed by Police. There has always been a high volume of police killings, although damning videos, photos, and news reports highlight officer violence — especially against people of color — now more than ever. But what’s become an even more alarming trend is the number of officers involved in these killings who receive minor to no punishment. According to the According to the Wall Street Journal , 2015 saw the highest number of police officers being charged for deadly, on-duty shootings in a decade: 12 as of September 2015. Still, in a year when approximately 1,200 people were killed by police, zero officers were convicted of murder or manslaughter, painting the picture that officers involved in killing another person will not be held accountable for their actions.\"\n",
    "text0 =\"Videos show the most delightful protest everAustralians know how to protest. Hundreds of people gathered Saturday local time at Parliament House in Canberra to make their way down a hill in a mass protest roll. The government plans to build a security fence to block access to the hill and other capital grounds. Protesters opposed to the fence rolled down the grassy slope just as many visitors to Parliament House often do. Even dogs got in on the democratic action. The event was organized by Lester Yao, an architect, on Facebook and delightful videos of the roll-a-thon were shared widely on social media. It was only going to be about 20 friends and families, and now we had more than 600 or 700 people, Yao told the Sydney Morning Herald. Unfortunately, kids might not be able to do this again and they're just enjoying themselves. The fence became a matter of debate after demonstrators breached security at Parliament House earlier this year. Lawmakers had even tossed around the idea of digging a moat around the slope, but that was sanely rejected.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15dcafab",
   "metadata": {},
   "source": [
    "Testing models on sample validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bc9ccd1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'LABEL_0', 'score': 0.9987004995346069}]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_model(text0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dc928856",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'LABEL_1', 'score': 0.9718432426452637}]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_model(text1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
